{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e878deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a6d810b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn import feature_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8f5e2380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_afl_data():\n",
    "    df_2017 = pd.read_csv(\"../data/afl_results_2017.csv\")\n",
    "    #print(df_2017.shape)\n",
    "    df_2018 = pd.read_csv(\"../data/afl_results_2018.csv\")\n",
    "    #print(df_2018.shape)\n",
    "    df_2019 = pd.read_csv(\"../data/afl_results_2019.csv\")\n",
    "    #print(df_2019.shape)\n",
    "    df_2020 = pd.read_csv(\"../data/afl_results_2020.csv\")\n",
    "    #print(df_2020.shape)\n",
    "    df_2021 = pd.read_csv(\"../data/afl_results_2021.csv\")\n",
    "    #print(df_2021.shape)\n",
    "    df_2022 = pd.read_csv(\"../data/afl_results_2022.csv\")\n",
    "    #print(df_2022.shape)\n",
    "    df_all = pd.concat([df_2017, df_2018, df_2019, df_2020, df_2021,df_2022], axis=0)\n",
    "    df_all['Date'] = pd.to_datetime(df_all['match.date']).dt.strftime(\"%Y-%m-%d\")\n",
    "    df_players_2017 = pd.read_csv(\"../data/afl_players_stats_2017.csv\")\n",
    "    #print(df_players_2017.shape)\n",
    "    df_players_2018 = pd.read_csv(\"../data/afl_players_stats_2018.csv\")\n",
    "    #print(df_players_2018.shape)\n",
    "    df_players_2019 = pd.read_csv(\"../data/afl_players_stats_2019.csv\")\n",
    "    #print(df_players_2019.shape)\n",
    "    df_players_2020 = pd.read_csv(\"../data/afl_players_stats_2020.csv\")\n",
    "    #print(df_players_2020.shape)\n",
    "    df_players_2021 = pd.read_csv(\"../data/afl_players_stats_2021.csv\")\n",
    "    #print(df_players_2021.shape)\n",
    "    df_players_2022 = pd.read_csv(\"../data/afl_players_stats_2022.csv\")\n",
    "    #print(df_players_2022.shape)\n",
    "    df_players = pd.concat([df_players_2017, df_players_2018, df_players_2019,df_players_2020,df_players_2021,df_players_2022], axis=0)\n",
    "    #print(df_players.shape)\n",
    "    #df_players.columns\n",
    "    df_fixture = pd.read_csv(\"../data/fixture_2022.csv\")\n",
    "    df_next_games_teams = df_fixture[(df_fixture['status'] != \"CONCLUDED\") & (df_fixture['round.roundNumber'] == 11)]\n",
    "    df_next_games_teams = df_next_games_teams[['home.team.name','away.team.name','venue.name','compSeason.year','compSeason.currentRoundNumber']]\n",
    "    df_next_games_teams = df_next_games_teams.rename(columns={'home.team.name': 'match.homeTeam.name', 'away.team.name': 'match.awayTeam.name','compSeason.year':'round.year','compSeason.currentRoundNumber':'round.roundNumber'})\n",
    "    df_next_games_teams['match.matchId'] = np.arange(len(df_next_games_teams))\n",
    "    \n",
    "    return df_all, df_players, df_fixture, df_next_games_teams\n",
    "\n",
    "def get_aggregate_player_stats(df=None):\n",
    "\n",
    "    agg_stats = (df.rename(columns={ # Rename columns to lowercase\n",
    "                    'Home.team': 'match.homeTeam.name',\n",
    "                    'Away.team': 'match.awayTeam.name',\n",
    "                    })\n",
    "                   .groupby(by=['Date', 'Season', 'match.homeTeam.name', 'match.awayTeam.name'], as_index=False) # Groupby to aggregate the stats for each game\n",
    "                   .sum()\n",
    "                   #.drop(columns=['DE', 'TOG', 'Match_id']) # Drop columns\n",
    "                   .assign(date=lambda df: pd.to_datetime(df.Date, format=\"%Y-%m-%d\")) # Create a datetime object\n",
    "                   .sort_values(by='Date')\n",
    "                   .reset_index(drop=True))\n",
    "    return agg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2852b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1072, 123)\n"
     ]
    }
   ],
   "source": [
    "df_all, df_players, df_fixture, df_next_games_teams = load_afl_data()\n",
    "agg_player = get_aggregate_player_stats(df_players)\n",
    "afl_df = df_all.merge(agg_player, on=['Date', 'match.homeTeam.name', 'match.awayTeam.name'], how='left')\n",
    "print(afl_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "38089d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add average goal diff for home and away team rolling 4 games\n",
    "\n",
    "afl_df['HTGDIFF'] = afl_df['homeTeamScore.matchScore.goals'] - afl_df['awayTeamScore.matchScore.goals']\n",
    "afl_df['ATGDIFF'] = afl_df['awayTeamScore.matchScore.goals'] - afl_df['homeTeamScore.matchScore.goals']\n",
    "\n",
    "def from_dict_value_to_df(d):\n",
    "    \"\"\"\n",
    "    input = dictionary \n",
    "    output = dataframe as part of all the values from the dictionary\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for v in d.values():\n",
    "        df = pd.concat([df,v])\n",
    "    return df\n",
    "\n",
    "def avg_goal_diff(df, avg_h_a_diff, a_h_team, a_h_goal_letter):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        df = dataframe with all results\n",
    "        avg_h_a_diff = name of the new column\n",
    "        a_h_team = HomeTeam or AwayTeam\n",
    "        a_h_goal_letter = 'H' for home or 'A' for away\n",
    "    output: \n",
    "        avg_per_team = dictionary with with team as key and columns as values with new column H/ATGDIFF\n",
    "    \"\"\"\n",
    "    df[avg_h_a_diff] = 0\n",
    "    avg_per_team = {}\n",
    "    all_teams = df[a_h_team].unique()\n",
    "    for t in all_teams:\n",
    "        df_team = df[df[a_h_team]==t].fillna(0)\n",
    "        result = df_team['{}TGDIFF'.format(a_h_goal_letter)].rolling(4).mean()\n",
    "        df_team[avg_h_a_diff] = result\n",
    "        avg_per_team[t] = df_team\n",
    "    return avg_per_team\n",
    "\n",
    "d_AVGFTHG = avg_goal_diff(afl_df, 'AVGHTGDIFF', 'match.homeTeam.name', 'H')\n",
    "df_AVGFTHG = from_dict_value_to_df(d_AVGFTHG)\n",
    "df_AVGFTHG.sort_index(inplace=True)\n",
    "d_AVGFTAG = avg_goal_diff(df_AVGFTHG, 'AVGATGDIFF', 'match.awayTeam.name', 'A')\n",
    "afl_df = from_dict_value_to_df(d_AVGFTAG)\n",
    "afl_df.sort_index(inplace=True)\n",
    "afl_df['AVGATGDIFF'].fillna(0, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0d2c5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "afl_df['goal_diff'] = afl_df['homeTeamScore.matchScore.goals'] - afl_df['awayTeamScore.matchScore.goals']\n",
    "\n",
    "for index, row in df_all[df_all['match.status']=='CONCLUDED'].iterrows():\n",
    "    if afl_df['goal_diff'][index] > 0:\n",
    "        afl_df.at[index,'result'] = 1   # 1 is a win\n",
    "    else:\n",
    "        afl_df.at[index,'result'] = 0  # 0 is a loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b4805587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_data(df, h_or_a_team, column, letter, past_n):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        df = dataframe with all results\n",
    "        a_h_team = HomeTeam or AwayTeam\n",
    "        column = column selected to get previous data from\n",
    "    output:\n",
    "        team_with_past_dict = dictionary with team as a key and columns as values with new \n",
    "                              columns with past value\n",
    "    \"\"\"\n",
    "    d = dict()\n",
    "    team_with_past_dict = dict()\n",
    "    all_teams = df[h_or_a_team].unique()\n",
    "    for team in all_teams:\n",
    "        n_games = len(df[df[h_or_a_team]==team])\n",
    "        team_with_past_dict[team] = df[df[h_or_a_team]==team]\n",
    "        for i in range(1, past_n):\n",
    "            d[i] = team_with_past_dict[team].assign(\n",
    "                result=team_with_past_dict[team].groupby(h_or_a_team)[column].shift(i)\n",
    "            ).fillna({'{}_X'.format(column): 0})\n",
    "            team_with_past_dict[team]['{}_{}_{}'.format(letter, column, i)] = d[i].result\n",
    "    return team_with_past_dict\n",
    "\n",
    "def previous_data_call(df, side, column, letter, iterations):\n",
    "    d = previous_data(df, side, column, letter, iterations)\n",
    "    df_result= from_dict_value_to_df(d)\n",
    "    df_result.sort_index(inplace=True)\n",
    "    return df_result\n",
    "\n",
    "df_last_home_results = previous_data_call(afl_df, 'match.homeTeam.name', 'result', 'H', 3)\n",
    "df_last_away_results = previous_data_call(df_last_home_results, 'match.awayTeam.name', 'result', 'A', 3)\n",
    "df_last_last_HTGDIFF_results = previous_data_call(df_last_away_results, 'match.homeTeam.name', 'HTGDIFF', 'H', 3)\n",
    "df_last_last_ATGDIFF_results = previous_data_call(df_last_last_HTGDIFF_results, 'match.awayTeam.name', 'ATGDIFF', 'A', 3)\n",
    "df_last_AVGFTHG_results = previous_data_call(df_last_last_ATGDIFF_results, 'match.homeTeam.name', 'AVGHTGDIFF', 'H', 2)\n",
    "df_last_AVGFTAG_results = previous_data_call(df_last_AVGFTHG_results, 'match.awayTeam.name', 'AVGATGDIFF', 'A', 2)\n",
    "afl_df = df_last_AVGFTAG_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f266e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_cols = ['match.matchId','match.date', 'match.status', 'match.venue', 'match.homeTeam.name', 'match.awayTeam.name','venue.name', 'venue.state', 'round.name', 'round.year', 'round.roundNumber', 'status',\n",
    "'homeTeamScore.rushedBehinds', 'homeTeamScore.minutesInFront',\n",
    "       'homeTeamScore.matchScore.totalScore', 'homeTeamScore.matchScore.goals',\n",
    "       'homeTeamScore.matchScore.behinds',\n",
    "       'homeTeamScore.matchScore.superGoals', 'awayTeamScore.rushedBehinds',\n",
    "       'awayTeamScore.minutesInFront', 'awayTeamScore.matchScore.totalScore',\n",
    "       'awayTeamScore.matchScore.goals', 'awayTeamScore.matchScore.behinds',\n",
    "       'awayTeamScore.matchScore.superGoals', 'weather.tempInCelsius',\n",
    "       'homeTeamScoreChart.goals', 'homeTeamScoreChart.leftBehinds',\n",
    "       'homeTeamScoreChart.rightBehinds', 'homeTeamScoreChart.leftPosters',\n",
    "       'homeTeamScoreChart.rightPosters', 'homeTeamScoreChart.rushedBehinds',\n",
    "       'homeTeamScoreChart.touchedBehinds', 'awayTeamScoreChart.goals',\n",
    "       'awayTeamScoreChart.leftBehinds', 'awayTeamScoreChart.rightBehinds',\n",
    "       'awayTeamScoreChart.leftPosters', 'awayTeamScoreChart.rightPosters',\n",
    "       'awayTeamScoreChart.rushedBehinds', 'awayTeamScoreChart.touchedBehinds', \n",
    "       'HQ1G', 'HQ1B', 'HQ2G',\n",
    "       'HQ2B', 'HQ3G', 'HQ3B', 'HQ4G', 'HQ4B', 'Home.score', 'AQ1G', 'AQ1B',\n",
    "       'AQ2G', 'AQ2B', 'AQ3G', 'AQ3B', 'AQ4G', 'AQ4B', 'Away.score',\n",
    "       'Kicks', 'Marks', 'Handballs', 'Goals', 'Behinds', 'Hit.Outs',\n",
    "       'Tackles', 'Rebounds', 'Inside.50s', 'Clearances', 'Clangers',\n",
    "       'Frees.For', 'Frees.Against', 'Brownlow.Votes', 'Contested.Possessions',\n",
    "       'Uncontested.Possessions', 'Contested.Marks', 'Marks.Inside.50',\n",
    "       'One.Percenters', 'Bounces', 'Goal.Assists', 'Time.on.Ground..',\n",
    "       'Substitute', 'group_id', 'HTGDIFF', 'ATGDIFF', 'AVGHTGDIFF',\n",
    "       'AVGATGDIFF', 'goal_diff', 'result', 'H_result_1', 'H_result_2',\n",
    "       'A_result_1', 'A_result_2', 'H_HTGDIFF_1', 'H_HTGDIFF_2', 'A_ATGDIFF_1',\n",
    "       'A_ATGDIFF_2', 'H_AVGHTGDIFF_1', 'A_AVGATGDIFF_1']\n",
    "\n",
    "non_feature_cols = ['match.matchId','match.date', 'match.status', 'match.venue', 'match.homeTeam.name', 'match.awayTeam.name','venue.name', 'venue.state', 'round.name', 'round.year', 'round.roundNumber', 'status','Season']\n",
    "feature_cols = [\n",
    "       'homeTeamScore.rushedBehinds', 'homeTeamScore.minutesInFront',\n",
    "       'homeTeamScore.matchScore.totalScore', 'homeTeamScore.matchScore.goals',\n",
    "       'homeTeamScore.matchScore.behinds',\n",
    "       'homeTeamScore.matchScore.superGoals', 'awayTeamScore.rushedBehinds',\n",
    "       'awayTeamScore.minutesInFront', 'awayTeamScore.matchScore.totalScore',\n",
    "       'awayTeamScore.matchScore.goals', 'awayTeamScore.matchScore.behinds',\n",
    "       'awayTeamScore.matchScore.superGoals', 'weather.tempInCelsius',\n",
    "       'homeTeamScoreChart.goals', 'homeTeamScoreChart.leftBehinds',\n",
    "       'homeTeamScoreChart.rightBehinds', 'homeTeamScoreChart.leftPosters',\n",
    "       'homeTeamScoreChart.rightPosters', 'homeTeamScoreChart.rushedBehinds',\n",
    "       'homeTeamScoreChart.touchedBehinds', 'awayTeamScoreChart.goals',\n",
    "       'awayTeamScoreChart.leftBehinds', 'awayTeamScoreChart.rightBehinds',\n",
    "       'awayTeamScoreChart.leftPosters', 'awayTeamScoreChart.rightPosters',\n",
    "       'awayTeamScoreChart.rushedBehinds', 'awayTeamScoreChart.touchedBehinds', \n",
    "       'HQ1G', 'HQ1B', 'HQ2G',\n",
    "       'HQ2B', 'HQ3G', 'HQ3B', 'HQ4G', 'HQ4B', 'Home.score', 'AQ1G', 'AQ1B',\n",
    "       'AQ2G', 'AQ2B', 'AQ3G', 'AQ3B', 'AQ4G', 'AQ4B', 'Away.score',\n",
    "       'Kicks', 'Marks', 'Handballs', 'Goals', 'Behinds', 'Hit.Outs',\n",
    "       'Tackles', 'Rebounds', 'Inside.50s', 'Clearances', 'Clangers',\n",
    "       'Frees.For', 'Frees.Against', 'Brownlow.Votes', 'Contested.Possessions',\n",
    "       'Uncontested.Possessions', 'Contested.Marks', 'Marks.Inside.50',\n",
    "       'One.Percenters', 'Bounces', 'Goal.Assists', 'Time.on.Ground..',\n",
    "       'Substitute', 'group_id', 'HTGDIFF', 'ATGDIFF', 'AVGHTGDIFF',\n",
    "       'AVGATGDIFF', 'goal_diff', 'result', 'H_result_1', 'H_result_2',\n",
    "       'A_result_1', 'A_result_2', 'H_HTGDIFF_1', 'H_HTGDIFF_2', 'A_ATGDIFF_1',\n",
    "       'A_ATGDIFF_2', 'H_AVGHTGDIFF_1', 'A_AVGATGDIFF_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56bf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f15ad1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "afl_df = afl_df[all_cols] \n",
    "\n",
    "afl_df = afl_df.rename(columns={col: 'f_' + col for col in afl_df if col not in non_feature_cols})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "29eebd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new round data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dbfbcedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_training_and_test_data(afl_df,df_next_games_teams):\n",
    "    \n",
    "    # Define a function which returns a DataFrame with the expontential moving average for each numeric stat\n",
    "    def create_exp_weighted_avgs(df, span):\n",
    "        # Create a copy of the df with only the game id and the team - we will add cols to this df\n",
    "        ema_features = df[['match.matchId', 'match.homeTeam.name']].copy()\n",
    "\n",
    "        feature_names = [col for col in df.columns if col.startswith('f_')] # Get a list of columns we will iterate over\n",
    "\n",
    "        for feature_name in feature_names:\n",
    "            feature_ema = (df.groupby('match.homeTeam.name')[feature_name]\n",
    "                             .transform(lambda row: (row.ewm(span=span)\n",
    "                                                        .mean()\n",
    "                                                        .shift(1))))\n",
    "            ema_features[feature_name] = feature_ema\n",
    "\n",
    "        return ema_features\n",
    "    \n",
    "    \n",
    "    afl_df['train_data'] = 1\n",
    "    df_next_games_teams['train_data'] = 0\n",
    "    \n",
    "    afl_data = afl_df.append(df_next_games_teams).reset_index(drop=True)\n",
    "    \n",
    "    features_rolling_averages = create_exp_weighted_avgs(afl_data, span=10)\n",
    "    \n",
    "    features = afl_data[['match.date', 'match.matchId', 'match.homeTeam.name', 'match.awayTeam.name', 'venue.name','round.year','train_data']].copy()\n",
    "    features = pd.merge(features, features_rolling_averages, on=['match.matchId', 'match.homeTeam.name'])\n",
    "    \n",
    "    form_btwn_teams = afl_df[['match.matchId', 'match.homeTeam.name', 'match.awayTeam.name', 'f_goal_diff']].copy()\n",
    "\n",
    "\n",
    "#    form_btwn_teams_inv = pd.DataFrame()\n",
    "\n",
    "#    for index, row in form_btwn_teams.iterrows():\n",
    "#        home = row['match.homeTeam.name']\n",
    "#        away = row['match.awayTeam.name']\n",
    "#        matchid = row['match.matchId']\n",
    "#        margin = row['f_goal_diff']\n",
    "\n",
    "#        form_btwn_teams_inv = form_btwn_teams_inv.append({'match.matchId': matchid, 'match.homeTeam.name': away, 'match.awayTeam.name': home, 'f_goal_diff': -1*margin}, ignore_index=True)\n",
    "\n",
    "#    form_btwn_teams['f_form_margin_btwn_teams'] = (form_btwn_teams.groupby(['match.homeTeam.name', 'match.awayTeam.name'])['f_goal_diff']\n",
    "#                                                              .transform(lambda row: row.rolling(5).mean().shift())\n",
    "#                                                              .fillna(0))\n",
    "\n",
    "#    form_btwn_teams['f_form_past_5_btwn_teams'] = \\\n",
    "#    (form_btwn_teams.assign(win=lambda df: df.apply(lambda row: 1 if row.f_goal_diff > 0 else 0, axis='columns'))\n",
    "#                  .groupby(['match.homeTeam.name', 'match.awayTeam.name'])['win']\n",
    "#                  .transform(lambda row: row.rolling(5).mean().shift() * 5)\n",
    "#                  .fillna(0))\n",
    "\n",
    "\n",
    "    #print(features.shape)\n",
    "    # Merge to our features df\n",
    "    #features = pd.merge(features, form_btwn_teams_1.drop(columns=['f_goal_diff']), on=['match.matchId', 'match.homeTeam.name', 'match.awayTeam.name'])\n",
    "    #print(features.shape)\n",
    "\n",
    "\n",
    "    # Get the result and merge to the feature_df\n",
    "\n",
    "    match_results = (afl_df.assign(result=lambda df: df.apply(lambda row: 1 if row['f_goal_diff'] > 0 else 0, axis=1)))\n",
    "    # Merge result column to feature_df\n",
    "    feature_df = pd.merge(features, match_results[['match.matchId', 'result']], on='match.matchId')\n",
    "\n",
    "    return feature_df,features_rolling_averages, afl_data, features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c55e2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df, features_rolling_averages, afl_data, features = create_training_and_test_data(afl_df,df_next_games_teams)\n",
    "feature_columns = [col for col in feature_df if col.startswith('f_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4ea13fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build model from feature_df\n",
    "\n",
    "feature_df = feature_df.dropna()\n",
    "\n",
    "all_X = feature_df.loc[:, feature_columns]\n",
    "all_y = feature_df.loc[:, 'result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train[feature_columns] = scaler.fit_transform(X_train[feature_columns])\n",
    "X_test[feature_columns] = scaler.transform(X_test[feature_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "27867792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of standard classifiers\n",
    "classifiers = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "]\n",
    "\n",
    "# Define a functiom which finds the best algorithms for our modelling task\n",
    "def find_best_algorithms(classifier_list, X, y):\n",
    "    # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n",
    "    # Cross validate model with Kfold stratified cross validation\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Grab the cross validation scores for each algorithm\n",
    "    cv_results = [cross_val_score(classifier, X, y, scoring = \"neg_log_loss\", cv = kfold) for classifier in classifier_list]\n",
    "    cv_means = [cv_result.mean() * -1 for cv_result in cv_results]\n",
    "    cv_std = [cv_result.std() for cv_result in cv_results]\n",
    "    algorithm_names = [alg.__class__.__name__ for alg in classifiers]\n",
    "    \n",
    "    # Create a DataFrame of all the CV results\n",
    "    cv_results = pd.DataFrame({\n",
    "        \"Mean Log Loss\": cv_means,\n",
    "        \"Log Loss Std\": cv_std,\n",
    "        \"Algorithm\": algorithm_names\n",
    "    })\n",
    "    \n",
    "    \n",
    "    return cv_results.sort_values(by='Mean Log Loss').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3145ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:23:16] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:16] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:16] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:16] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:16] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Log Loss</th>\n",
       "      <th>Log Loss Std</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674814</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>SVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.675778</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.681123</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>NuSVC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.683161</td>\n",
       "      <td>0.013955</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.691307</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.692060</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.692239</td>\n",
       "      <td>0.021653</td>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.711392</td>\n",
       "      <td>0.034796</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.741099</td>\n",
       "      <td>0.008702</td>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.924799</td>\n",
       "      <td>0.042759</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.932108</td>\n",
       "      <td>0.046650</td>\n",
       "      <td>BaggingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.871758</td>\n",
       "      <td>0.271105</td>\n",
       "      <td>BernoulliNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.173764</td>\n",
       "      <td>0.090322</td>\n",
       "      <td>GaussianNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.892970</td>\n",
       "      <td>1.581468</td>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean Log Loss  Log Loss Std                      Algorithm\n",
       "0        0.674814      0.005833                            SVC\n",
       "1        0.675778      0.005249           LogisticRegressionCV\n",
       "2        0.681123      0.008026                          NuSVC\n",
       "3        0.683161      0.013955         RandomForestClassifier\n",
       "4        0.691307      0.000986             AdaBoostClassifier\n",
       "5        0.692060      0.000595      GaussianProcessClassifier\n",
       "6        0.692239      0.021653           ExtraTreesClassifier\n",
       "7        0.711392      0.034796     GradientBoostingClassifier\n",
       "8        0.741099      0.008702     LinearDiscriminantAnalysis\n",
       "9        0.924799      0.042759                  XGBClassifier\n",
       "10       0.932108      0.046650              BaggingClassifier\n",
       "11       1.871758      0.271105                    BernoulliNB\n",
       "12       2.173764      0.090322                     GaussianNB\n",
       "13      10.892970      1.581468  QuadraticDiscriminantAnalysis"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_algos = find_best_algorithms(classifiers, X_train, y_train)\n",
    "best_algos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a52ccee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5739983337120351"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)\n",
    "cv_scores = cross_val_score(ensemble.RandomForestClassifier(), X_train, y_train, scoring='accuracy', cv=kfold)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "211d224f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5726804514125577"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a logistic regression model and see how it performs in terms of accuracy\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "cv_scores = cross_val_score(linear_model.LogisticRegressionCV(), X_train, y_train, scoring='accuracy', cv=kfold)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5134dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:23:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[08:23:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.609566007725517"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5)\n",
    "cv_scores = cross_val_score(XGBClassifier(), X_train, y_train, scoring='accuracy', cv=kfold)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80069c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d30eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7a3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "577f8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which optimises the hyperparameters of our chosen algorithms\n",
    "def optimise_hyperparameters(train_x, train_y, algorithms, parameters):\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    best_estimators = []\n",
    "    \n",
    "    for alg, params in zip(algorithms, parameters):\n",
    "        gs = GridSearchCV(alg, param_grid=params, cv=kfold, scoring='neg_log_loss', verbose=1)\n",
    "        gs.fit(train_x, train_y)\n",
    "        best_estimators.append(gs.best_estimator_)\n",
    "    return best_estimators\n",
    "\n",
    "# Define our parameters to run a grid search over\n",
    "lr_grid = {\n",
    "    \"C\": [0.0001, 0.001, 0.01, 0.05, 0.2, 0.5],\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "\n",
    "# Add our algorithms and parameters to lists to be used in our function\n",
    "alg_list = [LogisticRegression(), XGBClassifier(), ensemble.RandomForestClassifier()]\n",
    "param_list = [lr_grid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8d94b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "# Find the best estimators, then add our other estimators which don't need optimisation\n",
    "best_estimators = optimise_hyperparameters(X_train, y_train, alg_list, param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e6b5f98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_best_params = best_estimators[0].get_params()\n",
    "lr_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "82c1946e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6748134683492715"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)\n",
    "cv_scores = cross_val_score(linear_model.LogisticRegression(**lr_best_params), X_train, y_train, scoring='neg_log_loss', cv=kfold)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "111e2d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy in predicting test data is: 63.55%\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(**lr_best_params)\n",
    "lr.fit(X_train, y_train)\n",
    "final_predictions = lr.predict(X_test)\n",
    "\n",
    "accuracy = (final_predictions == y_test).mean() * 100\n",
    "\n",
    "print(\"Our accuracy in predicting test data is: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "db36cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_scorer(y,y_pred):\n",
    "    xx = pd.DataFrame({'y':list(y), 'y_pred': list(y_pred)}, columns=['y','y_pred'])     \n",
    "    xx.loc[xx['y'].astype(int)==0,'actual_score'] = 1+np.log2(1-xx['y_pred'].astype(float))\n",
    "   \n",
    "    xx.loc[xx['y'].astype(int)==1,'actual_score'] = 1+np.log2(xx['y_pred'].astype(float))\n",
    "   \n",
    "    return np.mean(xx['actual_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "76b516eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_scorer(y_test,final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "100812ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41005355, 0.58994645],\n",
       "       [0.52128916, 0.47871084],\n",
       "       [0.36214413, 0.63785587],\n",
       "       [0.4175295 , 0.5824705 ],\n",
       "       [0.56547132, 0.43452868],\n",
       "       [0.54901047, 0.45098953],\n",
       "       [0.50424763, 0.49575237],\n",
       "       [0.44711202, 0.55288798],\n",
       "       [0.60853961, 0.39146039],\n",
       "       [0.65670322, 0.34329678],\n",
       "       [0.547716  , 0.452284  ],\n",
       "       [0.45375979, 0.54624021],\n",
       "       [0.51047059, 0.48952941],\n",
       "       [0.55008883, 0.44991117],\n",
       "       [0.55679383, 0.44320617],\n",
       "       [0.65019857, 0.34980143],\n",
       "       [0.53606148, 0.46393852],\n",
       "       [0.42877736, 0.57122264],\n",
       "       [0.40986462, 0.59013538],\n",
       "       [0.49257596, 0.50742404],\n",
       "       [0.49725997, 0.50274003],\n",
       "       [0.38866162, 0.61133838],\n",
       "       [0.43767266, 0.56232734],\n",
       "       [0.61574258, 0.38425742],\n",
       "       [0.41670737, 0.58329263],\n",
       "       [0.45680665, 0.54319335],\n",
       "       [0.46708644, 0.53291356],\n",
       "       [0.44525136, 0.55474864],\n",
       "       [0.33108743, 0.66891257],\n",
       "       [0.51834943, 0.48165057],\n",
       "       [0.47216214, 0.52783786],\n",
       "       [0.4789503 , 0.5210497 ],\n",
       "       [0.48182486, 0.51817514],\n",
       "       [0.39126044, 0.60873956],\n",
       "       [0.50163425, 0.49836575],\n",
       "       [0.55173194, 0.44826806],\n",
       "       [0.38617554, 0.61382446],\n",
       "       [0.48351543, 0.51648457],\n",
       "       [0.3941725 , 0.6058275 ],\n",
       "       [0.34686709, 0.65313291],\n",
       "       [0.59733958, 0.40266042],\n",
       "       [0.58546289, 0.41453711],\n",
       "       [0.49957409, 0.50042591],\n",
       "       [0.51777068, 0.48222932],\n",
       "       [0.42614699, 0.57385301],\n",
       "       [0.55846994, 0.44153006],\n",
       "       [0.49226369, 0.50773631],\n",
       "       [0.53153627, 0.46846373],\n",
       "       [0.452216  , 0.547784  ],\n",
       "       [0.45346079, 0.54653921],\n",
       "       [0.40577193, 0.59422807],\n",
       "       [0.50173811, 0.49826189],\n",
       "       [0.5360375 , 0.4639625 ],\n",
       "       [0.56584519, 0.43415481],\n",
       "       [0.36046332, 0.63953668],\n",
       "       [0.5740089 , 0.4259911 ],\n",
       "       [0.46014429, 0.53985571],\n",
       "       [0.51870087, 0.48129913],\n",
       "       [0.54027353, 0.45972647],\n",
       "       [0.56065046, 0.43934954],\n",
       "       [0.60602203, 0.39397797],\n",
       "       [0.56207976, 0.43792024],\n",
       "       [0.48206476, 0.51793524],\n",
       "       [0.68204565, 0.31795435],\n",
       "       [0.51300991, 0.48699009],\n",
       "       [0.54509983, 0.45490017],\n",
       "       [0.51225493, 0.48774507],\n",
       "       [0.46783227, 0.53216773],\n",
       "       [0.44689398, 0.55310602],\n",
       "       [0.52936138, 0.47063862],\n",
       "       [0.46881993, 0.53118007],\n",
       "       [0.57019624, 0.42980376],\n",
       "       [0.41287621, 0.58712379],\n",
       "       [0.4579117 , 0.5420883 ],\n",
       "       [0.59100674, 0.40899326],\n",
       "       [0.58013812, 0.41986188],\n",
       "       [0.38165638, 0.61834362],\n",
       "       [0.42299152, 0.57700848],\n",
       "       [0.46239207, 0.53760793],\n",
       "       [0.51922914, 0.48077086],\n",
       "       [0.67765891, 0.32234109],\n",
       "       [0.50901816, 0.49098184],\n",
       "       [0.3317052 , 0.6682948 ],\n",
       "       [0.65252292, 0.34747708],\n",
       "       [0.42305342, 0.57694658],\n",
       "       [0.41195726, 0.58804274],\n",
       "       [0.44707616, 0.55292384],\n",
       "       [0.39388546, 0.60611454],\n",
       "       [0.51531809, 0.48468191],\n",
       "       [0.59121994, 0.40878006],\n",
       "       [0.55521765, 0.44478235],\n",
       "       [0.42816515, 0.57183485],\n",
       "       [0.43050543, 0.56949457],\n",
       "       [0.44235621, 0.55764379],\n",
       "       [0.46903831, 0.53096169],\n",
       "       [0.48315742, 0.51684258],\n",
       "       [0.59782444, 0.40217556],\n",
       "       [0.52857313, 0.47142687],\n",
       "       [0.50295951, 0.49704049],\n",
       "       [0.48764432, 0.51235568],\n",
       "       [0.3822204 , 0.6177796 ],\n",
       "       [0.42172374, 0.57827626],\n",
       "       [0.45818941, 0.54181059],\n",
       "       [0.54609055, 0.45390945],\n",
       "       [0.41736995, 0.58263005],\n",
       "       [0.58505952, 0.41494048],\n",
       "       [0.40200515, 0.59799485],\n",
       "       [0.57432285, 0.42567715],\n",
       "       [0.56005606, 0.43994394],\n",
       "       [0.50666481, 0.49333519],\n",
       "       [0.54719104, 0.45280896],\n",
       "       [0.40572429, 0.59427571],\n",
       "       [0.43723172, 0.56276828],\n",
       "       [0.55923062, 0.44076938],\n",
       "       [0.32759941, 0.67240059],\n",
       "       [0.46168493, 0.53831507],\n",
       "       [0.48739197, 0.51260803],\n",
       "       [0.50691404, 0.49308596],\n",
       "       [0.42537807, 0.57462193],\n",
       "       [0.51492331, 0.48507669],\n",
       "       [0.36547982, 0.63452018],\n",
       "       [0.43968621, 0.56031379],\n",
       "       [0.46837406, 0.53162594],\n",
       "       [0.50042035, 0.49957965],\n",
       "       [0.47109622, 0.52890378],\n",
       "       [0.47773966, 0.52226034],\n",
       "       [0.42711574, 0.57288426],\n",
       "       [0.51396445, 0.48603555],\n",
       "       [0.57259946, 0.42740054],\n",
       "       [0.48224621, 0.51775379],\n",
       "       [0.50605183, 0.49394817],\n",
       "       [0.45926141, 0.54073859],\n",
       "       [0.47777372, 0.52222628],\n",
       "       [0.49017688, 0.50982312],\n",
       "       [0.43656058, 0.56343942],\n",
       "       [0.44467657, 0.55532343],\n",
       "       [0.55774573, 0.44225427],\n",
       "       [0.47051087, 0.52948913],\n",
       "       [0.46020437, 0.53979563],\n",
       "       [0.54674569, 0.45325431],\n",
       "       [0.57843752, 0.42156248],\n",
       "       [0.44227866, 0.55772134],\n",
       "       [0.41144683, 0.58855317],\n",
       "       [0.55662717, 0.44337283],\n",
       "       [0.48209388, 0.51790612],\n",
       "       [0.4916409 , 0.5083591 ],\n",
       "       [0.53819784, 0.46180216],\n",
       "       [0.5854591 , 0.4145409 ],\n",
       "       [0.45507207, 0.54492793],\n",
       "       [0.50627198, 0.49372802],\n",
       "       [0.53938372, 0.46061628],\n",
       "       [0.44263069, 0.55736931],\n",
       "       [0.39143723, 0.60856277],\n",
       "       [0.5119449 , 0.4880551 ],\n",
       "       [0.5416911 , 0.4583089 ],\n",
       "       [0.54492383, 0.45507617],\n",
       "       [0.55619056, 0.44380944],\n",
       "       [0.6609628 , 0.3390372 ],\n",
       "       [0.58168004, 0.41831996],\n",
       "       [0.66295346, 0.33704654],\n",
       "       [0.40469329, 0.59530671],\n",
       "       [0.61895008, 0.38104992],\n",
       "       [0.44844633, 0.55155367],\n",
       "       [0.46675324, 0.53324676],\n",
       "       [0.50107672, 0.49892328],\n",
       "       [0.54324619, 0.45675381],\n",
       "       [0.46300932, 0.53699068],\n",
       "       [0.5428747 , 0.4571253 ],\n",
       "       [0.51084918, 0.48915082],\n",
       "       [0.44512903, 0.55487097],\n",
       "       [0.46693111, 0.53306889],\n",
       "       [0.53913251, 0.46086749],\n",
       "       [0.49784687, 0.50215313],\n",
       "       [0.64837799, 0.35162201],\n",
       "       [0.66081523, 0.33918477],\n",
       "       [0.39012412, 0.60987588],\n",
       "       [0.38369995, 0.61630005],\n",
       "       [0.48704107, 0.51295893],\n",
       "       [0.58644697, 0.41355303],\n",
       "       [0.50104342, 0.49895658],\n",
       "       [0.57830474, 0.42169526],\n",
       "       [0.44324474, 0.55675526],\n",
       "       [0.56435201, 0.43564799],\n",
       "       [0.46012822, 0.53987178],\n",
       "       [0.57585685, 0.42414315],\n",
       "       [0.52634465, 0.47365535],\n",
       "       [0.51435243, 0.48564757],\n",
       "       [0.43529901, 0.56470099],\n",
       "       [0.50165682, 0.49834318],\n",
       "       [0.59577105, 0.40422895],\n",
       "       [0.55252355, 0.44747645],\n",
       "       [0.57131136, 0.42868864],\n",
       "       [0.45816618, 0.54183382],\n",
       "       [0.59245739, 0.40754261],\n",
       "       [0.48273628, 0.51726372],\n",
       "       [0.5670024 , 0.4329976 ],\n",
       "       [0.56318964, 0.43681036],\n",
       "       [0.38609435, 0.61390565],\n",
       "       [0.38765708, 0.61234292],\n",
       "       [0.56654687, 0.43345313],\n",
       "       [0.44702663, 0.55297337],\n",
       "       [0.49141069, 0.50858931],\n",
       "       [0.53834647, 0.46165353]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_probs = lr.predict_proba(X_test)\n",
    "prediction_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "222c33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_round_features = features[features['train_data']==0][feature_columns]\n",
    "\n",
    "next_round_predictions = lr.predict(next_round_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ef575ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_round_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bde313d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37351025, 0.62648975],\n",
       "       [0.06392282, 0.93607718],\n",
       "       [0.15687094, 0.84312906],\n",
       "       [0.10554245, 0.89445755],\n",
       "       [0.93549549, 0.06450451],\n",
       "       [0.74288048, 0.25711952],\n",
       "       [0.86059803, 0.13940197],\n",
       "       [0.99747831, 0.00252169],\n",
       "       [0.99708279, 0.00291721]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_probs = lr.predict_proba(next_round_features)\n",
    "prediction_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12912fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_scorer(y_test,next_round_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2320132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
